# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "marimo",
#     "pyspark",
#     "pyiceberg[pyarrow,s3fs]",
#     "pandas",
# ]
# ///

import marimo

__generated_with = "0.18.4"
app = marimo.App(width="medium")


@app.cell
def _():
    import marimo as mo
    mo.md("""
    # ğŸš€ ãƒãƒ«ãƒã‚¨ãƒ³ã‚¸ãƒ³æ¤œè¨¼: PySpark + PyIceberg

    ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€**PySpark**ã¨**PyIceberg**ã§åŒã˜Icebergãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€
    ãƒãƒ«ãƒã‚¨ãƒ³ã‚¸ãƒ³ã§ã®ç›¸äº’é‹ç”¨æ€§ã‚’ç¢ºèªã—ã¾ã™ã€‚

    ## æ¤œè¨¼ãƒã‚¤ãƒ³ãƒˆ
    1. PyIcebergã§ä½œæˆã—ãŸãƒ†ãƒ¼ãƒ–ãƒ«ã‚’Sparkã§èª­ã¿å–ã‚Œã‚‹ã‹
    2. Sparkã§æ›¸ãè¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ã‚’PyIcebergã§èª­ã¿å–ã‚Œã‚‹ã‹
    3. ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®å…±æœ‰
    """)
    return (mo,)


@app.cell
def _(mo):
    mo.md("""
    ## Part 1: PySpark ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä½œæˆ

    REST CatalogçµŒç”±ã§Icebergãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹Sparkã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¾ã™ã€‚
    """)
    return


@app.cell
def _():
    from pyspark.sql import SparkSession
    import os

    # Sparkè¨­å®š
    # JARã¯PySparkã®jarsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®æ¸ˆã¿ã®ãŸã‚ã€spark.jarsè¨­å®šã¯ä¸è¦
    spark = SparkSession.builder \
        .appName("IcebergMultiEngine") \
        .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
        .config("spark.sql.catalog.rest", "org.apache.iceberg.spark.SparkCatalog") \
        .config("spark.sql.catalog.rest.type", "rest") \
        .config("spark.sql.catalog.rest.uri", os.environ.get("CATALOG_URI", "http://rest-catalog:8181")) \
        .config("spark.sql.catalog.rest.io-impl", "org.apache.iceberg.aws.s3.S3FileIO") \
        .config("spark.sql.catalog.rest.s3.endpoint", os.environ.get("S3_ENDPOINT", "http://minio:9000")) \
        .config("spark.sql.catalog.rest.s3.access-key-id", os.environ.get("AWS_ACCESS_KEY_ID", "admin")) \
        .config("spark.sql.catalog.rest.s3.secret-access-key", os.environ.get("AWS_SECRET_ACCESS_KEY", "password")) \
        .config("spark.sql.catalog.rest.s3.path-style-access", "true") \
        .config("spark.sql.defaultCatalog", "rest") \
        .config("spark.driver.memory", "1g") \
        .getOrCreate()

    print(f"Spark version: {spark.version}")
    print("Spark ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆå®Œäº†")
    return (spark,)


@app.cell
def _(mo):
    mo.md("""
    ## Part 2: PyIcebergã§ä½œæˆã—ãŸãƒ†ãƒ¼ãƒ–ãƒ«ã‚’Sparkã§èª­ã‚€

    å‰ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ï¼ˆ01_pyiceberg_intro.pyï¼‰ã§ä½œæˆã—ãŸ `demo.users` ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’
    Sparkã§èª­ã¿å–ã‚Šã¾ã™ã€‚
    """)
    return


@app.cell
def _(spark):
    # Sparkã§ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã‚’ç¢ºèª
    spark.sql("SHOW NAMESPACES").show()
    return


@app.cell
def _(spark):
    # demoåå‰ç©ºé–“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§
    spark.sql("SHOW TABLES IN demo").show()
    return


@app.cell
def _(spark):
    # PyIcebergã§ä½œæˆã—ãŸãƒ†ãƒ¼ãƒ–ãƒ«ã‚’Sparkã§èª­ã¿å–ã‚Š
    df_spark = spark.sql("SELECT * FROM rest.demo.users")
    df_spark.show()
    return


@app.cell
def _(mo):
    mo.md("""
    ## Part 3: Sparkã§ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 

    Sparkã‹ã‚‰æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã€PyIcebergã§èª­ã¿å–ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚
    """)
    return


@app.cell
def _(spark):
    # Sparkã§ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    spark.sql("""
        INSERT INTO rest.demo.users VALUES
        (4, 'David', 'david@example.com', 88.0),
        (5, 'Eve', 'eve@example.com', 95.5)
    """)
    print("Sparkã‹ã‚‰2ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
    return


@app.cell
def _(spark):
    # è¿½åŠ å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªï¼ˆSparkï¼‰
    spark.sql("SELECT * FROM rest.demo.users ORDER BY user_id").show()
    return


@app.cell
def _(mo):
    mo.md("""
    ## Part 4: PyIcebergã§è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª

    Sparkã§è¿½åŠ ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’PyIcebergã§èª­ã¿å–ã‚Œã‚‹ã‹ç¢ºèªã—ã¾ã™ã€‚
    """)
    return


@app.cell
def _():
    from pyiceberg.catalog import load_catalog
    import os as os2

    # PyIcebergã§ã‚«ã‚¿ãƒ­ã‚°ã«æ¥ç¶š
    catalog = load_catalog(
        "rest",
        **{
            "type": "rest",
            "uri": os2.environ.get("CATALOG_URI", "http://rest-catalog:8181"),
            "s3.endpoint": os2.environ.get("S3_ENDPOINT", "http://minio:9000"),
            "s3.access-key-id": os2.environ.get("AWS_ACCESS_KEY_ID", "admin"),
            "s3.secret-access-key": os2.environ.get("AWS_SECRET_ACCESS_KEY", "password"),
            "s3.region": "us-east-1",
        }
    )

    # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ï¼‰
    table = catalog.load_table("demo.users")
    table.refresh()

    # PyIcebergã§ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã‚Š
    df_pyiceberg = table.scan().to_pandas()
    df_pyiceberg
    return (table,)


@app.cell
def _(mo):
    mo.md("""
    ## Part 5: ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®ç¢ºèª

    PyIcebergã¨Sparkã®ä¸¡æ–¹ã‹ã‚‰æ“ä½œã—ãŸçµæœã€ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãŒã©ã®ã‚ˆã†ã«
    è¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¾ã™ã€‚
    """)
    return


@app.cell
def _(mo, table):
    # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå±¥æ­´
    snapshots_data = []
    for snap in table.metadata.snapshots:
        app_id = snap.summary.get("app-id") or ""
        snapshots_data.append({
            "Snapshot ID": snap.snapshot_id,
            "Operation": snap.summary.get("operation", "N/A"),
            "Added Records": snap.summary.get("added-records", "0"),
            "Engine": "PyIceberg" if "pyiceberg" in app_id.lower() else "Spark",
        })

    mo.ui.table(snapshots_data)
    return


@app.cell
def _(mo):
    mo.md("""
    ## Part 6: Sparkã§ã‚¹ã‚­ãƒ¼ãƒé€²åŒ–

    Sparkã‚’ä½¿ã£ã¦ã‚«ãƒ©ãƒ ã‚’è¿½åŠ ã—ã€ã‚¹ã‚­ãƒ¼ãƒé€²åŒ–ã‚’ç¢ºèªã—ã¾ã™ã€‚
    """)
    return


@app.cell
def _(spark):
    # ã‚«ãƒ©ãƒ ã‚’è¿½åŠ 
    spark.sql("ALTER TABLE rest.demo.users ADD COLUMNS (created_at TIMESTAMP)")
    print("ã‚«ãƒ©ãƒ  'created_at' ã‚’è¿½åŠ ã—ã¾ã—ãŸ")

    # ã‚¹ã‚­ãƒ¼ãƒã‚’ç¢ºèª
    spark.sql("DESCRIBE rest.demo.users").show()
    return


@app.cell
def _(table):
    # PyIcebergã§ã‚‚ã‚¹ã‚­ãƒ¼ãƒå¤‰æ›´ã‚’ç¢ºèª
    table.refresh()

    print("=== PyIcebergã§ã‚¹ã‚­ãƒ¼ãƒã‚’ç¢ºèª ===")
    for field in table.schema().fields:
        print(f"  {field.field_id}: {field.name} ({field.field_type}) required={field.required}")
    return


@app.cell
def _(mo):
    mo.md("""
    ---

    ## ã¾ã¨ã‚

    âœ… **ãƒãƒ«ãƒã‚¨ãƒ³ã‚¸ãƒ³ç›¸äº’é‹ç”¨æ€§ã‚’ç¢ºèªã—ã¾ã—ãŸ**

    | æ“ä½œ | PyIceberg | Spark |
    |------|-----------|-------|
    | ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ | âœ… | âœ… |
    | ãƒ‡ãƒ¼ã‚¿èª­ã¿å–ã‚Š | âœ… | âœ… |
    | ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿ | âœ… | âœ… |
    | ã‚¹ã‚­ãƒ¼ãƒé€²åŒ– | âœ… | âœ… |
    | ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå…±æœ‰ | âœ… | âœ… |

    REST Catalogã‚’ä»‹ã™ã‚‹ã“ã¨ã§ã€ç•°ãªã‚‹ã‚¨ãƒ³ã‚¸ãƒ³ãŒåŒã˜Icebergãƒ†ãƒ¼ãƒ–ãƒ«ã«
    ä¸€è²«ã—ã¦ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã“ã¨ãŒç¢ºèªã§ãã¾ã—ãŸã€‚
    """)
    return


@app.cell
def _():
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
    # spark.stop()
    return


if __name__ == "__main__":
    app.run()
